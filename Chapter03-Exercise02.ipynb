{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 3: Resampling Methods\n",
    "# Exercise 2: Bootstrapping (Under Sampling Minority Class) \n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To become familiar with Bootstrapping. You will use only train/test as we did in Chapter 1 and 2, not cross-validation.\n",
    "\n",
    " \n",
    "It aligns with all the learning outcome of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Intructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the standardized training and test data, and the hospital data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data inside the folder data/diabetes/CSV_data.zip and place it inside the folder data/diabetes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "#For this notebook to work, Python must be 3.6.4 or 3.6.5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital = pd.read_csv('data/diabetes/Data_Class_Dummies.csv', sep=',')\n",
    "train_standardized_data = pd.read_csv('data/diabetes/train_standardized_data.csv', sep=',')\n",
    "test_standardized_data = pd.read_csv('data/diabetes/test_standardized_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(hospital[:][:5])\n",
    "hospital.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(train_standardized_data[:][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(test_standardized_data[:][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Split the training and test data into features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_standardized = train_standardized_data.drop(['readmission'], axis = 1)\n",
    "y_train = train_standardized_data[['readmission']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_standardized = test_standardized_data.drop(['readmission'], axis = 1)\n",
    "y_test = test_standardized_data[['readmission']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_standardized.shape)\n",
    "print(X_test_standardized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Binarise response\n",
    "We will be using the f1 score at various points in this exercise. So let's create a binary response for the training and test response vectors we have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training response:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks:\n",
    "print('******************************************')\n",
    "#print(y_train)\n",
    "print('y_train - NO values =', sum(i =='NO' for i in y_train))\n",
    "print('y_train - YES values =', sum(i =='YES' for i in y_train))\n",
    "print('******************************************\\n')\n",
    "\n",
    "# Create y_train_binary\n",
    "y_train_binary = [0 if x=='NO' else 1 for x in y_train]\n",
    "\n",
    "\n",
    "# Sanity Check\n",
    "print('A few elements of y_train: ', y_train[:12].ravel())\n",
    "print('Corresponding elements of y_train_binary: ', y_train_binary[:12])\n",
    "\n",
    "# Sanity Checks:\n",
    "print('\\n******************************************')\n",
    "#print(y_train)\n",
    "print('y_train_binary - 0 values =', sum(i ==0 for i in y_train_binary))\n",
    "print('y_train - 1 values =', sum(i ==1 for i in y_train_binary))\n",
    "print('******************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test response:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks:\n",
    "print('******************************************')\n",
    "#print(y_test)\n",
    "print('y_test - NO values =', sum(i =='NO' for i in y_test))\n",
    "print('y_test - YES values =', sum(i =='YES' for i in y_test))\n",
    "print('******************************************\\n')\n",
    "\n",
    "# Create y_test_binary\n",
    "y_test_binary = [0 if x=='NO' else 1 for x in y_test]\n",
    "\n",
    "\n",
    "# Sanity Check\n",
    "print('A few elements of y_test: ', y_test[:12].ravel())\n",
    "print('Corresponding elements of y_test_binary: ', y_test_binary[:12])\n",
    "\n",
    "# Sanity Checks:\n",
    "print('\\n******************************************')\n",
    "#print(y_test)\n",
    "print('y_test_binary - 0 values =', sum(i ==0 for i in y_test_binary))\n",
    "print('y_test - 1 values =', sum(i ==1 for i in y_test_binary))\n",
    "print('******************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative fast method of finding unique values and counts within a numpy array:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_test, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_test_binary, return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bootstrapping\n",
    "Bootstrapping is a broad term referring to any statistical method that utilises **random sampling with replacement**.<p>\n",
    "**Bootstrap Sample:** Say we have *n* data points in our sample. A bootstrap sample of this data set is generated by drawing a data point with replacement exactly *n* times. The result is a sample of the same size as the original, but with duplicates. How many duplicates? Well, on *average* approximately 1/3 of the original data points will be excluded from our bootstrap sample. That is, approximately 2/3 of the original data will be included.<p>\n",
    "    Proof: Suppose the original data contains n observations. The probability that a particular observation is not chosen from a set of n observations is $1 - {1\\over n}$, so the probability that the observation is not chosen n times is $(1 - {1\\over n})^n$. This is the probability that the observation does not appear in a bootstrap sample. You may recall that the Eulers number, $e$, is defined as $e := \\lim_{n\\to\\infty}{(1 + {1\\over n})^n}$. From this fact, and assuming we have a relatively large sample size, we can show that the probability that the observation does not appear in a bootstrap sample is equal to $1\\over e$, which is approximately $1\\over 3$. \n",
    "\n",
    "<p> **Example:** A classic example of bootstrapping is to determine the variance of a test statistic (such as the mean). The method involves first obtaining a dataset of size $n$. Then, for some large number $B$, generate $B$ new datasets of size $n$ by repeatedly sampling with replacement from the original dataset. That is, we generate $B$ bootstrap samples. Then, for each of these $B$ bootstrap samples, we compute the test statistic (for example, the mean). This will give us an empirical sampling distribution of the test statistic, which provides us with the variance. Crucially, this method gives us the variance without any need of a formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Oversampling minority class, readmission = YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our goal is to address the imbalanced response problem by evening the class label distribution. In this first method, we will <b>'oversample the minority class'</b>. This means we will generate extra data points corresponding to the 'YES' class label. The method for doing so is outlined in [this website](https://blog.dominodatalab.com/imbalanced-datasets/).<p>\n",
    "    The name of the method is Synthetic Minority Oversampling TEchnique (SMOTE). In short, SMOTE involves:\n",
    "* Randomly sample a data point from the minority group (readmission = YES, in our case).\n",
    "* For some choice of k, compute the k nearest neighbours of this point.\n",
    "* Add k new points somewhere between the chosen point and its k nearest neighbours.\n",
    "* Repeat process until the classes are even.\n",
    "\n",
    "As you can see, the SMOTE method combines bootstrapping and k-nearest neighbour to synthetically create additional observations of the minority class, in this case readmission = 'YES'. However, it does so in such a way to ensure that each new synthetic child sample is never an exact duplicate of its parents.<p>\n",
    "![alt text](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png 'SMOTE Visualisation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Resample the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the package 'imbalanced-learn'. This contains several useful resampling functions. It may require the installation of 'msgpack'.<p>\n",
    "    The function we will use is from imblearn.over_sampling, and is called SMOTE. Read more about its details and parameter choices [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install msgpack\n",
    "# Alternative: type 'pip install msgpack' in Ananconda prompt (Windows) or the command line (Mac/Linux)\n",
    "\n",
    "# Install 'imbalanced-learn'\n",
    "!pip install -U imbalanced-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you obtain a message that says \"ModuleNotFoundError: No module named 'imblearn'\", try this:\n",
    "conda install -c glemaitre imbalanced-learn (run the line below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c glemaitre imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 1**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1a:  Write the SMOTE() function. Leave all arguments as default except random_state=0 and ratio? Which option would you choose for ratio?  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here\n",
    "# We will name our model 'smote'\n",
    "\n",
    "smote = SMOTE()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1b:  What does the SMOTE() function do?  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write the answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1c:  Write the fit_sample() function.   </font><p>\n",
    " <font color='red'>**NB** Note that the SMOTE function is fitted in the training set **ONLY**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here\n",
    "\n",
    "X_train_standardized_smote, y_train_smote = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1d:  What does the fit_sample() function do?  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write the answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 1**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_standardized_data['readmission'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For even ratio, need to add the following number of records to the YES class:\n",
    "print(train_standardized_data['readmission'].value_counts()[0]-train_standardized_data['readmission'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_train_smote, return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after performing the SMOTE algorithm, the number of NO and YES cases are now equal at 47,769."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 2**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2a: Fit a logistic regression model with L1-norm regularization (Lasso). Choose the value of C. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Python code here\n",
    "from ...\n",
    "\n",
    "Log_Reg = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2b: Print the beta coefficients  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Python code here\n",
    "\n",
    "# Beta Coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 2c: Show the name of the columns whose beta coefficients are different from zero  </font>\n",
    "<p><font color='green'> Tip: You can find the code in Chapter2 - Exercise 03 </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Python code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 2**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**NB** Note that the SMOTE function is fitted in the training set **ONLY**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions \n",
    "from sklearn import metrics\n",
    "y_pred= Log_Reg.predict(X_test_standardized)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = Log_Reg.score(X_test_standardized, y_test_binary)\n",
    "print('Accuracy: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Evaluating the model using F1 Score\n",
    "We will use the average F1 score between YES and NO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 scores need 'YES' to be 1 and 'NO' to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/F1score.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 3**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3: Calculate the F1-score  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type Python code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 3**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>classification_report</b> function produces one line per class (here, \"YES\" and\n",
    "\"NO\", or \"1\" and \"0\") and returns precision, recall, and f1-score with this class as the positive class.\n",
    "\n",
    "Before, we assumed the minority “1” class was the positive class. If we change the\n",
    "positive class to “0,” we can see from the output of <b>classification_report</b>\n",
    "that we obtain an f1-score of 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_binary, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the f1 score, macro averaged. The f1-score macro-averaged calculated the average f1-score of the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_true = y_test_binary, y_pred = y_pred, average = 'macro') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Receiver Operating Characteristic (ROC): TPR and FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Probability associated with each prediction\n",
    "We need to determine the probability of each record in the test set being a 'YES', or equivalently a 1 as we have converted the response into a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities of the test set being 0 and 1\n",
    "y_pred_proba = Log_Reg.predict_proba(X_test_standardized)[:,1]\n",
    "y_pred\n",
    "\n",
    "print(y_pred_proba[:5])\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. Determining the fpr, tpr at each threshold value\n",
    "Now that we have the probabilities associated with each prediction, we know exactly which records are predicted YES and NO for each choice of decision threshold. Hence, we can determine the false positive rate (fpr) and true positive rate (tpr) for threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_binary, y_pred_proba)\n",
    "print(fpr[:5])\n",
    "print(tpr[:5])\n",
    "print(thresholds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['fpr'] = fpr\n",
    "df['tpr'] = tpr\n",
    "# Sanity check \n",
    "display(df[:][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr,_= metrics.roc_curve(y_true = y_test_binary, y_score = y_pred_proba)\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "p = ggplot(mapping = aes(x = fpr, y = tpr), data = df)\n",
    "p += geom_line(color = 'red')\n",
    "p += geom_abline(aes(intercept=0, slope=1), linetype = 'dashed', colour = 'blue')\n",
    "p += labs(title = 'ROC Curve', x = 'fpr', y = 'tpr')\n",
    "p += theme_bw()\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4. Area under the ROC curve (AUC)\n",
    "Note that AUC = 0.5 corresponds to random assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.roc_auc_score(y_true = y_test_binary, y_score = y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Computing optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of pair that maximises tpr - fpr\n",
    "ind_max = np.argmax(tpr - fpr)\n",
    "print(ind_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold value that maximises the tpr - fpr\n",
    "optimal_thresh = thresholds[ind_max]\n",
    "print(optimal_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_test_binary, y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance Metrics\n",
    "Here, we will recap and introduce some functions to evaluate performance metrics we can use to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Confusion_Matrix.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = y_test_binary, y_pred = y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Balanced Accuracy\n",
    "Balanced accuracy, $\\phi$, is defined as the arithmetic mean of the class-specific accuracies:\n",
    "$$ \\phi := {1\\over2}(\\pi^+ + \\pi^-) ,$$\n",
    "Where $\\pi^+ = {TP\\over TP+FP}$ is the accuracy of the positive class (readmission = YES) and $\\pi^- = {TN\\over TN+FN}$ is the accuracy of the negative class (readmission = NO). If the classifier performs equally well for both classes, then balanced accuracy reduces to regular accuracy. However, balanced accuracy penalises classifiers that perform differently for each class.<p>\n",
    "    Now, the way we will calculate balanced accuracy in Python is via the confusion matrix. \n",
    "* TN is the first entry of the first column. FN is the second entry of the first column.\n",
    "* TP is the first entry of the second column. FP is the second entry of the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_pred, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = y_test_binary, y_pred = y_pred)\n",
    "print(\"Confusion matrix:\\n{}\".format(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of YES\n",
    "acc_pos = cm[1][1]/(cm[1][1] + cm[0][1])\n",
    "print(acc_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of NO\n",
    "acc_neg = cm[0][0]/(cm[0][0] + cm[1][0])\n",
    "print(acc_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Accuracy\n",
    "BACC = (acc_pos + acc_neg)/2\n",
    "print(BACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Precision\n",
    "pos_label specifies which class label we wish to calculate precision for.<p>\n",
    "average = 'macro' indicates that we wish to compute the unweighted mean for all classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_true = y_test_binary, y_pred = y_pred, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_true = y_test_binary, y_pred = y_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_true = y_test_binary, y_pred = y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_true = y_test_binary, y_pred = y_pred, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_true = y_test_binary, y_pred = y_pred, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_true = y_test_binary, y_pred = y_pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. precision_recall_fscore_support\n",
    "\n",
    "Read http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "This provides a summary of the precision, recall, f1-score and support. \n",
    "\n",
    "This is a convenient way of computing the precision, recall, fscore and support. The output is a list of arrays, the first array containing the precision of NO and YES, the second containing the recall of NO and YES, and the third containing the f1 score of NO and YES. The fourth array is the support of NO and YES, which is simply the number of occurences for each class label.\n",
    "\n",
    "The average is a prevalance-weighted average, meaning that the majority class has a larger influence than the minority. This is not useful for us, and there is no parameter to change it. Also, the table is only in text form, meaning we can't access the individual elements as if it were an array. This would make further analysis tedious, as we'd have to enter the numbers manually for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">**Start Activity 4**</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4a:  Write the function that calculates the classification_report() and save the value in a variable named \"some_metrics\" </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Python code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4b:  Define precision and recall  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write the answer here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">**End Activity 4**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of NO\n",
    "some_metrics[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall of YES\n",
    "some_metrics[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting average='macro', we obtain an un-weighted mean of precision, recall, and fscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_metrics = precision_recall_fscore_support(y_true = y_test_binary, y_pred = y_pred, average = 'macro')\n",
    "print(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Precision: {}'.format(average_metrics[0]))\n",
    "print('Average Recall: {}'.format(average_metrics[1]))\n",
    "print('Average f1 score: {}'.format(average_metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. F1 Score - Unweighted mean\n",
    "If we set average='macro', we obtain the unweighted mean of f1 score between the NO and YES class. This metric is useful, as it ensures that both classes are taken into consideration. We will use this averaged F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true = y_test_binary, y_pred = y_pred, average = 'macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
