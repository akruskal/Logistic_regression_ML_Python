{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/HDAT9500Banner.PNG)\n",
    "<br>\n",
    "\n",
    "# Chapter 3: Model Evaluation and Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####################################################################################\n",
    "\n",
    "Double-click to write down your name and surname.\n",
    "\n",
    "**Name:**\n",
    "\n",
    "\n",
    "**Surname:**\n",
    "\n",
    "**Honour Pledge** <p>\n",
    "    \n",
    "    \n",
    "Declaration: <p>\n",
    "    \n",
    "    \n",
    "I declare that this assessment item is my own work, except where acknowledged, and has not been submitted for academic credit elsewhere or previously, or produced independently of this course (e.g. for a third party such as your place of employment) and acknowledge that the assessor of this item may, for the purpose of assessing this item: \n",
    "\n",
    "    a. Reproduce this assessment item and provide a copy to another member of the University; and/or \n",
    "    b. Communicate a copy of this assessment item to a plagiarism checking service (which may then retain a copy of the assessment item on its database for the purpose of future plagiarism checking). \n",
    "\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment\n",
    "\n",
    "\n",
    "Tuning parameters with Grid + cross-validation: GridSearchCV <font color=green>**(Step 6 of the ML work-flow)**</font>\n",
    "\n",
    "Pipelines.\n",
    "\n",
    "The test will be kept in a \"safe box\" to use once we have found the best parameters and the best model with Grid SearchCV.\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "In this assessment, you will be asked to use 'Grid' to find the best $alpha$ ($alpha=C=1/\\lambda$) and the best combination of B:M class_weight for logistic regression with Ridge (L2) regularization and 5-CV.\n",
    "\n",
    "**NB Nomeclature: **\n",
    "Training, Validation and Test Set. \n",
    "\n",
    "* The training set, used to train the model\n",
    "* The validation set, used to evaluate model performance and adjust hyper-parameters accordingly (for example, the alpha for Ridge Regression). Therefore, the validation set is used as an intermediate step. \n",
    "* The test set, used for final model evaluation. Book 2 uses the term \"validation set\" for what we call \"test set\" in Book 1.\n",
    "\n",
    "\n",
    "## 1.1. Aims of the Exercise:\n",
    " 1. To become familiar with a validation set to find the best hyper-parameters of a model. Remember that the hyper-parameters are defined by the user.\n",
    " 2. To become familiar with a grid search: the most commonly used method for tuning parameters is via a grid search, which entails testing many combinations of the parameters of interest.\n",
    " 3. To become familiar with k-CV and grid search\n",
    " 4. To become familiar with Python pipelines\n",
    "\n",
    " \n",
    "It aligns with all of the learning outcomes of our course: \n",
    "\n",
    "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
    "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
    "3.\tConstruct appropriate training and test sets for health research data.\n",
    "\n",
    "\n",
    "## 1.2. Jupyter Notebook Instructions\n",
    "1. Read the content of each cell.\n",
    "2. Where necessary, follow the instructions that are written in each cell.\n",
    "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
    "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
    " \n",
    "## 1.3. Tips\n",
    " 1. The square brackets on the left hand side of each cell indicate whether the cell has been executed or not. Empty square brackets mean that the cell has not been executed, whereas square brackets that contain a number means that the cell has been executed. Run all of the cells in sequence, using the \"Run\" button.\n",
    " 2. To edit this notebook, just double-click in each cell. In the document, each cell can be a \"Code\" cell or \"text-Markdown\" cell. To choose between these two options, go to the combo-box above. \n",
    " 3. If you want to save your notebook, please make sure you press the \"floppy disk\" icon button above. \n",
    " 4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data/breast-cancer-wisconsin-data/data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:\n",
    "display(dataframe[:][:5])\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grid Search with Cross-Validation: GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used method for tuning parameters is via a grid search, which entails testing many combinations of the parameters of interest.<p>\n",
    "    \n",
    " We want to utilise the benefits of cross-validation with the grid search. We will seek to find the model with the best accuracy by using cross-validation. We will use the \"GridSearchCV\" class from sklearn.\n",
    " \n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "    \n",
    "We have two primary hyper-parameters that we would like to tune:\n",
    "* C, ($C=alpha=1/\\lambda$)\n",
    "* class_weight, the class weights.\n",
    "<p>\n",
    "    \n",
    "Let's say we want to try:\n",
    "\n",
    "1. C = 0.001, 0.01, 0.1, 1, 10, 100. And for \n",
    "\n",
    "2. class_weight = 'balanced', {'B':0.1, 'M':0.9}, {'B':0.2, 'M':0.8}, {'B':0.3, 'M':0.7}, {'B':0.4, 'M':0.6}, and {'B':0.5, 'M':0.5}. \n",
    "\n",
    "\n",
    "Note that class_weight = {'B':0.5, 'M':0.5} corresponds to no class weighting, as the weightings are equal. As there are 6 cases of C, and 6 of class weight, there are 6 times 6 = 36 total combinations of C and class weight.<p>\n",
    "    \n",
    "We will choose **L2 regularization (ridge) for this problem**. As we are using a grid, and later a grid in combination with cross-validation, we have to keep in our minds *computational complexity*. L2 has a closed form solution because it relies on squaring the beta coefficients. L1 does not have a closed form solution as it involves an absolute value. For this reason, L1 is computationally more expensive, as we can't solve it in terms of matrix math, and most rely on approximations (in the lasso case, coordinate descent). This means L2 will be much faster to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 1: Split the whole dataset into a train and a test set (20% of the total). Keep the test set aside (hidden inside a box as we mentioned in the videos) until the very end (15 marks)</font>\n",
    "\n",
    " <font color='green'> *NB*: We stratify in order to have the same number of classes in the different splits.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Write Python Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define the Pipeline\n",
    "\n",
    "As in exercise 1, we will have to use a pipeline in order to also standardize the features for each iteration of the cross-validation.\n",
    "\n",
    "\n",
    "### <font color='blue'> Question 2: Define the scaler we will use, and the estimator. As before, choose the scaler (\"Transform\") to be StandardScaler(), and the estimator (\"Estimator\") to be L2 Logistic Regression. (5 marks)</font>\n",
    "\n",
    "To read about Pipelines:\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "2. https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Write Python Code Here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Define the parameter grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 3: Define the parameter grid. This is the 2-dimensional range we wish to draw parameter values from. Call this object 'param_grid'. (20 marks)</font>\n",
    "\n",
    " **Important:** as we are using a pipeline, there are two processes that are executed for each iteration of the cross-validation. First, the standardisation, then the fitting of the logistic model. This means we have to indicate which of these processes our specified parameters should be used for. That is, the computer may try to fit class_weight into StandardScaler if we forget to tell it not to. Notice above that we have named our logistic model 'Estimator'. This means we can designate its parameters by naming the hyper-parameters in parameter grid \"Estimator__'parameter_name'\". **For example, we tell the computer that C is meant for the logistic regression estimator by defining it as 'Estimator__C' in the param_grid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Write Python Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 4: Initialise the GridSearchCV class by passing it the pipeline we have created, *pipe*, our paramater grid, *param_grid*, and specifying how many folds we would like. We must consider the computational complexity of the algorithm, so we can't set cv too high. We choose 5 folds.(5 marks)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Write Python Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 5: What score (performance metric) are we using in the GridSearchCV above? If we used the f1 score instead, what command or commands should be included? - (10 marks)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your thoughts here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Write Python code here\n",
    "grid_search_f1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Find the best parameters\n",
    "\n",
    "Now train the grid_search object. Note that grid_search behaves similarly to other classifiers, in the sense that we can use the methods fit, predict and score with it. When we use fit, it performs the grid cross-validation we designed during its initialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 6: Fit the GridSearchCV you created before and show the best parameters and score out of all the folds - (15 marks)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Write Python code here\n",
    "\n",
    "\n",
    "# It takes a while to run ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Visualise the grid results\n",
    "The results from the cross-validated grid search are stored in cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mglearn\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore') #prevent warnings\n",
    "\n",
    "# convert results to DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_) \n",
    "# show the first few rows \n",
    "display(results[:][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(results.mean_test_score)\n",
    "scores = scores.reshape(6, 6) \n",
    "# reshape: first index = number of values of C, second index = number of values of dict\n",
    "\n",
    "# Take transpose because we want class_weight on the y axis, so we can more easily see the tick labels\n",
    "scores = np.transpose(scores)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean cross-validation scores\n",
    "mglearn.tools.heatmap(scores, \n",
    "                      ylabel='class_weight', \n",
    "                      yticklabels=param_grid['Estimator__class_weight'], \n",
    "                      xlabel='C', \n",
    "                      xticklabels=param_grid['Estimator__C'], \n",
    "                      cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 7: Interpret the heatmap - (10 marks)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your thoughts here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Evaluate on the performance of the resulting model\n",
    "\n",
    "Recall that to this point we have not used the test set - only the training set was used for tuning the parameters. Now we will use confusion matrix and average f1 score to evaluate the model.\n",
    "\n",
    "\n",
    "\"Fitting the GridSearchCV object not only searches for the best parameters, but also\n",
    "automatically fits a new model on the whole training dataset with the parameters that\n",
    "yielded the best cross-validation performance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Question 8: Calculate confusion matrix, accuracy, recall, precision and f1 score. Comment on the results below - (20 marks)</font>\n",
    "<p><font color='green'> Tip: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Type Python code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Write your thoughts here:</b>\n",
    "#####################################################################################################################\n",
    "\n",
    "(Double-click here)\n",
    "\n",
    "\n",
    "#####################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
